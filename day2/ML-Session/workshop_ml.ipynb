{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Overview"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "\n",
    "  <div align=\"center\" style=\"margin: 25px 0; padding: 15px; background-color: white; border: 1px solid #444; border-radius: 8px;\">\n",
    "    <img src=\"./images/architecture.jpg\" alt=\"vNetRunner Overview\" width=\"300\"\" />\n",
    "  </div>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- Data Loading\n",
    "- Data Preprocessing\n",
    "    - Deleting columns with no variation\n",
    "    - Data normalization\n",
    "    - Removing the slient datapoints from other other slices\n",
    "    - Data Visualization\n",
    "\n",
    "- Model Training\n",
    "- Exercise: Window Size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm as normal\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.metrics import confusion_matrix as conf_mat\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigating the Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embb_csv = \"./logs/SingleUE/Raw/embb*.csv\"\n",
    "urrl_csv = \"./logs/SingleUE/Raw/urll*.csv\"\n",
    "mmtc_csv = \"./logs/SingleUE/Raw/mmtc*.csv\"\n",
    "clean_csv = \"./logs/SingleUE/Raw/null*.csv\"\n",
    "\n",
    "embb_files = glob(embb_csv)\n",
    "embb_logs = [pd.read_csv(f, sep=\",\").dropna(how='all', axis='columns') for f in embb_files]\n",
    "\n",
    "mmtc_files = glob(mmtc_csv)\n",
    "mmtc_logs = [pd.read_csv(f, sep=\",\").dropna(how='all', axis='columns') for f in mmtc_files]\n",
    "\n",
    "urll_files = glob(urrl_csv)\n",
    "urll_logs = [pd.read_csv(f, sep=\",\").dropna(how='all', axis='columns') for f in urll_files]\n",
    "\n",
    "ctrl_files = glob(clean_csv)\n",
    "ctrl_logs = [pd.read_csv(f, sep=\",\").dropna(how='all', axis='columns') for f in ctrl_files]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-family: Arial, sans-serif; color: #222; line-height: 1.6; max-width: 700px; margin: 20px auto; padding: 20px; border: 1px solid #444; border-radius: 8px; background-color: #eaeaea; box-shadow: 0px 4px 8px rgba(0, 0, 0, 0.15);\">\n",
    "  <h2 style=\"color: #ffffff; background-color: #333333; padding: 10px; border-radius: 4px; margin-bottom: 20px; text-align: center;\">Exercise: Explore the Dataset</h2>\n",
    "  \n",
    "  <p style=\"font-size: 1.1em; margin-top: 15px; margin-bottom: 20px;\">\n",
    "    Take a few minutes to examine the dataset. Look at the data structure, features, and any patterns that stand out. Once you are familiar with the dataset, try answering the following questions. To get familiar, we have included a the cell above. Uncomment suitable lines in the cell below to see the output. \n",
    "  </p>\n",
    "  \n",
    "  <h3 style=\"color: #ff9800; margin-bottom: 15px;\">Questions to Explore</h3>\n",
    "  <ol style=\"font-size: 1.1em; margin-left: 25px; margin-bottom: 20px;\">\n",
    "    <li>What do any of the variable classes represent? What are EMBB, MMTC and URLLC logs here?</li>\n",
    "    <li>What are the features of the each of the csv files we loaded?</li>\n",
    "    <li>These data are collected from an xApp. What was the sampling granularity of our data? <b>Hint:</b> look at the <code>Timestamp</code> column? (Hint: Uncomment the last line in the cell below)</li> \n",
    "  </ol>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center></center>\n",
    "\n",
    "<!-- \n",
    "Q1. For every type of traffic, we have collected a set of traces. For instace, for EMBB traces, we have `embb_logs` list where each item contains a tabular dataset for that EMBB trace. \n",
    "\n",
    "Q2. Each trace loaded is a python dataframe including 31 columns that are metrics of the UE collected every 250ms. We can see values like `dl_mcs`, `dl_buffer` size and so on. The min, max and standard deviation for values of every columns are also specified.\n",
    "\n",
    "Q3. As we have stated, data is collected every 250ms. So the data sampling granularity is 250ms. Now this data is sent from the RIC to the RAN not necesarily every 250ms, but possibly every few seconds. So even though the data is sampled every 250ms it is aggregated and sent from the RAN to RIC at lower granularities.\n",
    "\n",
    " -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment only one of the following lines to inspect one of the loaded datasets \n",
    "# embb_logs[0].describe()\n",
    "# mmtc_logs[0].describe()\n",
    "# urll_logs[0].describe()\n",
    "# ctrl_logs[0].describe()\n",
    "\n",
    "\n",
    "embb_logs[0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing some columns and Stacking the dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at only one row of data, we might not possibly say what the original application producing this datapoint was. \n",
    "Was it an eMBB application? or was it a URLLC application. But if we look at some continuous rows, we can have more clues about the original application. For this reason, we use the variable `window_size` to stack rows of the original dataset to form new datapoints. Each new datapoint will have information about `window_size` consecutive rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of previous rows to stack\n",
    "window_size = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also remove some columns that are irrelevant to traffic classification task.\n",
    "Furthermore, we aggregate all different traces of all different traffic classes into a single variable `data_in`. \n",
    "Plus, we store the class (label) of each datapoint in `data_lbl` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the this cell we drop some of the columns from each of the dataframes\n",
    "# Then, we stack window_size rows so each datapoint itself, is represents a consecutive window_size datapoints of the original dataset\n",
    "# We then have concatenated the dataframes of different types into one array with their corresponding labels in\n",
    "# `trails_in` and `trials_lbl` variables\n",
    "\n",
    "\n",
    "embb_traces = []\n",
    "embb_labels = []\n",
    "mmtc_traces = []\n",
    "mmtc_labels = []\n",
    "urll_traces = []\n",
    "urll_labels = []\n",
    "ctrl_traces = []\n",
    "ctrl_labels = []\n",
    "\n",
    "\n",
    "columns_drop = \"Timestamp num_ues IMSI RNTI slicing_enabled slice_id slice_prb scheduling_policy\".split(\" \")\n",
    "ctrl_class = 3\n",
    "\n",
    "# stacks `window_siz`e number of rows on top of each other to form a dataseries\n",
    "def make_windows(ds, window_size):\n",
    "    new_ds = []\n",
    "    for i in range(ds.shape[0]):\n",
    "        if i + window_size < ds.shape[0]:\n",
    "            new_ds.append(\n",
    "                ds[i:i + window_size]  # slice\n",
    "            )\n",
    "    new_ds = np.array(new_ds)\n",
    "    return new_ds\n",
    "\n",
    "\n",
    "def check_slices(data, index, check_zeros=False):\n",
    "    labels = np.ones((data.shape[0],), dtype=np.int32)*index\n",
    "    if not check_zeros:\n",
    "        return labels\n",
    "    for i in range(data.shape[0]):\n",
    "        sl = data[i]\n",
    "        zeros = (sl == 0).astype(int).sum(axis=1)\n",
    "        if (zeros > 10).all():\n",
    "            labels[i] = 3 # control if all KPIs rows have > 10 zeros\n",
    "    return labels\n",
    "\n",
    "\n",
    "for ix, ds in enumerate([embb_logs, mmtc_logs, urll_logs, ctrl_logs]):\n",
    "    for trace in ds:\n",
    "        all_cols_names = trace.columns.values\n",
    "\n",
    "        # We drop some of the columns here \n",
    "        columns_dropped = trace.drop(columns_drop, axis=1, inplace=False)\n",
    "        cols_names = columns_dropped.columns.values                        # after drop\n",
    "        new_trace = make_windows(columns_dropped, window_size)\n",
    "            \n",
    "        if ix == 0: # eMBB class\n",
    "            embb_traces.append(new_trace)\n",
    "            embb_labels.append(check_slices(new_trace, ix, False))         # labels are numbers (i.e. no 1 hot encoded)\n",
    "        elif ix == 1: # MMTC class\n",
    "            mmtc_traces.append(new_trace)\n",
    "            mmtc_labels.append(check_slices(new_trace, ix, False))\n",
    "        elif ix == 2: # URLLC class\n",
    "            urll_traces.append(new_trace)\n",
    "            urll_labels.append(check_slices(new_trace, ix, False))\n",
    "        elif ix == 3:\n",
    "            ctrl_traces.append(new_trace)\n",
    "            ctrl_labels.append(np.ones((new_trace.shape[0],), dtype=np.int32) * ix)\n",
    "\n",
    "data_in = np.concatenate((np.concatenate(embb_traces),\n",
    "                            np.concatenate(mmtc_traces),\n",
    "                            np.concatenate(urll_traces),\n",
    "                            np.concatenate(ctrl_traces)), axis=0).astype(np.float32)\n",
    "data_lbl = np.concatenate((np.concatenate(embb_labels),\n",
    "                             np.concatenate(mmtc_labels),\n",
    "                             np.concatenate(urll_labels),\n",
    "                             np.concatenate(ctrl_labels)), axis=0).astype(int)\n",
    "\n",
    "\n",
    "# Question: What is the dimension of the `data_in` variable? \n",
    "# What does the output of this cell show?\n",
    "\n",
    "\n",
    "\n",
    "print(f\"dataset shape: {data_in.shape}, labels shape: {data_lbl.shape}\")\n",
    "\n",
    "print(\"Number of datapoints:\", data_in.shape[0])\n",
    "print(\"size of each datapoint:\", data_in.shape[1:])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize the values of each column to a range of [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we look at all input features, and normalize the corresponding values to a range of [0, 1]\n",
    "# This enhances the performance of the machine learning models as they work better when the range of \n",
    "# input values is in [0, 1]\n",
    "\n",
    "\n",
    "def extract_feats_stats(cols_names, din):\n",
    "    columns_maxmin = {}\n",
    "    for c in range(din.shape[2]):\n",
    "        col_max = din[:, :, c].max()\n",
    "        col_min = din[:, :, c].min()\n",
    "        col_mean = din[:, :, c].mean()\n",
    "        col_std = din[:, :, c].std()\n",
    "        columns_maxmin[c] = {'max': col_max, 'min': col_min, 'mean': col_mean, 'std': col_std, 'name': cols_names[c]}\n",
    "    return columns_maxmin\n",
    "\n",
    "\n",
    "def normalize_KPIs(columns_maxmin, din, doPrint=False):\n",
    "    trials_in_norm = din.copy()\n",
    "    for c, max_min_info in columns_maxmin.items():\n",
    "        if isinstance(c, int):\n",
    "            if max_min_info['name'] != 'Timestamp':\n",
    "                col_max = max_min_info['max']\n",
    "                col_min = max_min_info['min']\n",
    "                if not (col_max == col_min):\n",
    "                    if doPrint:\n",
    "                        print('Normalizing Col.', max_min_info['name'], ' -- Max', col_max, ', Min', col_min)\n",
    "                    trials_in_norm[:, :, c] = (trials_in_norm[:, :, c] - col_min) / (col_max - col_min)\n",
    "                else:\n",
    "                    trials_in_norm[:, :, c] = 0  # set all data as zero (we don't need this info cause it never changes)\n",
    "            else:\n",
    "                if doPrint:\n",
    "                    print('Skipping normalization of Col. ', max_min_info['name'])\n",
    "\n",
    "    return trials_in_norm\n",
    "\n",
    "columns_maxmin = extract_feats_stats(cols_names, data_in)\n",
    "data_in_norm = normalize_KPIs(columns_maxmin, data_in, doPrint=True)\n",
    "\n",
    "print(f\"X shape: {data_in.shape}, Y shape: {data_lbl.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove columns with no variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_variation_col_idx = []\n",
    "no_variation_col_name = []\n",
    "for k, v in columns_maxmin.items():\n",
    "    if v['std'] == 0.0:\n",
    "\n",
    "\n",
    "        print(f\"feature {v['name']} removed because of its std is 0\" )\n",
    "        no_variation_col_idx.append(k)\n",
    "        no_variation_col_name.append(v['name'])\n",
    "\n",
    "trials_in = data_in.copy()\n",
    "trials_in = np.delete(trials_in, no_variation_col_idx, axis=-1)\n",
    "trials_in_norm = np.delete(data_in_norm, no_variation_col_idx, axis=-1)\n",
    "new_cols_names = np.delete(cols_names, no_variation_col_idx)\n",
    "columns_maxmin = extract_feats_stats(new_cols_names, trials_in)\n",
    "\n",
    "print(f\"New X shape: {trials_in.shape}, Y shape: {data_lbl.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffle the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the common steps in ml data processing is to divide the data into two splits. The training dataset and the evaluation dataset. The training dataset is used for training the ml model and the evaluation dataset is used for evluating the trained model so that model is evaluated on datapoints it has not seen. \n",
    "\n",
    "By shuffling the data, we make sure a different subset of points fall into the training and test splits every time we run the notebook from the top to bottom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samp_ixs = list(range(trials_in.shape[0]))\n",
    "np.random.shuffle(samp_ixs)                       # permutation\n",
    "trials_in = trials_in[samp_ixs, :, :]\n",
    "trials_in_norm = trials_in_norm[samp_ixs, :, :]\n",
    "trials_lbl = data_lbl[samp_ixs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spliting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_validation_ratio = 0.8\n",
    "\n",
    "n_samps = trials_in.shape[0]\n",
    "n_train = int(n_samps * train_validation_ratio)\n",
    "n_valid = n_samps - n_train\n",
    "nclasses = 3\n",
    "\n",
    "trials_ds = {\n",
    "    'train': {\n",
    "        'samples': {\n",
    "            'norm': torch.Tensor(trials_in_norm[0:n_train])\n",
    "        },\n",
    "        'labels': torch.Tensor(trials_lbl[0:n_train]).type(torch.LongTensor)\n",
    "    },\n",
    "    'valid': {\n",
    "        'samples': {\n",
    "            'norm': torch.Tensor(trials_in_norm[n_train:n_train+n_valid])\n",
    "        },\n",
    "        'labels': torch.Tensor(trials_lbl[n_train:n_train+n_valid]).type(torch.LongTensor)\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"Train X shape: {trials_ds['train']['samples']['norm'].shape}, Y shape: {trials_ds['train']['labels'].shape}\")\n",
    "print(f\"Test X shape: {trials_ds['valid']['samples']['norm'].shape}, Y shape: {trials_ds['valid']['labels'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detecting control traffic among other traffic\n",
    "\n",
    "eMBB, URLLC, and mMTC, and include an additional class of samples, denoted as control (ctrl), or “silent”, class to identify the portions of traffic where no application data is being exchanged between registered UEs and gNB\n",
    "\n",
    "\n",
    "Depending on the UE’s activities, control class traffic might be found in any of the three traffic categories and therefore we consider ctrl as a fourth meta-class that can be used to identify idle users and applications\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ctrl = None\n",
    "columns_maxmin['info'] = {}\n",
    "\n",
    "\n",
    "# find all data labeled as the control_class\n",
    "for t in ['train', 'valid']:\n",
    "    ixs_ctrl = trials_ds[t]['labels'] == ctrl_class\n",
    "    if torch.any(ixs_ctrl):\n",
    "        if all_ctrl is None:\n",
    "            all_ctrl = trials_ds[t]['samples']['norm'][ixs_ctrl]\n",
    "        else:\n",
    "            all_ctrl = torch.cat((all_ctrl, trials_ds[t]['samples']['norm'][ixs_ctrl]), dim=0)\n",
    "\n",
    "if (not (all_ctrl is None)) and (all_ctrl.shape[0] > 0):\n",
    "    mean_ctrl_sample = torch.mean(all_ctrl, dim=0)\n",
    "    std_ctrl_sample = torch.std(all_ctrl, dim=0)\n",
    "    columns_maxmin['info']['mean_ctrl_sample'] = mean_ctrl_sample\n",
    "    columns_maxmin['info']['std_ctrl_sample'] = std_ctrl_sample\n",
    "    columns_maxmin['info']['norm_dist'] = {}\n",
    "    x_axis = np.arange(0, 15, 0.01)\n",
    "    for cl in [0, 1, 2, 3]:\n",
    "            all_sample = None\n",
    "            for t in ['train', 'valid']:\n",
    "                ixs_ctrl = trials_ds[t]['labels'] == cl\n",
    "                if torch.any(ixs_ctrl):\n",
    "                    if all_sample is None:\n",
    "                        all_sample = trials_ds[t]['samples']['norm'][ixs_ctrl]\n",
    "                    else:\n",
    "                        all_sample = torch.cat((all_sample, trials_ds[t]['samples']['norm'][ixs_ctrl]), dim=0)\n",
    "\n",
    "            norm = np.linalg.norm(all_sample - mean_ctrl_sample, axis=(1, 2))\n",
    "            columns_maxmin['info']['norm_dist'][cl] = {'mean': np.mean(norm), 'std': np.std(norm)}\n",
    "\n",
    "ctrl_distrib = normal.pdf(x_axis, columns_maxmin['info']['norm_dist'][3]['mean'], columns_maxmin['info']['norm_dist'][3]['std'])\n",
    "for cl, traff_name in {0: 'eMBB', 1: 'mMTC', 2: 'URLLC'}.items():\n",
    "    this_mean, this_std = columns_maxmin['info']['norm_dist'][cl]['mean'], columns_maxmin['info']['norm_dist'][cl]['std']\n",
    "    this_distrib = normal.pdf(x_axis, this_mean, this_std)\n",
    "    thr_val = this_mean - (1 * this_std)\n",
    "    columns_maxmin['info']['norm_dist'][cl]['thr'] = thr_val # save threshold value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relabeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Make sure you run this cell only once or run everything from the beginning again\n",
    "\n",
    "\n",
    "for t in ['train', 'valid']:\n",
    "    include_ixs = set(range(trials_ds[t]['samples']['norm'].shape[-1]))\n",
    "    mean_ctrl_sample = columns_maxmin['info']['mean_ctrl_sample']\n",
    "    obs_excludecols = trials_ds[t]['samples']['norm'][:, :, list(include_ixs)]\n",
    "    norm = np.linalg.norm(obs_excludecols - mean_ctrl_sample, axis=(1, 2))\n",
    "    possible_ctrl_ixs = norm < columns_maxmin['info']['norm_dist'][ctrl_class]['mean']\n",
    "    possible_ctrl_labels = trials_ds[t]['labels'][possible_ctrl_ixs]\n",
    "    pre_unique_lbls, pre_unique_cnts = np.unique(trials_ds[t]['labels'], return_counts=True)\n",
    "    print(\"\\ntrain: Initial # samps. per label (before relabeling)\\n\\t Labels:\", pre_unique_lbls[:4], \"Count:\", pre_unique_cnts[:4])\n",
    "    unique_labels, unique_counts = np.unique(possible_ctrl_labels, return_counts=True)\n",
    "    count_relabel = 0\n",
    "    for ix, isPossibleCTRL in enumerate(possible_ctrl_ixs):\n",
    "        if isPossibleCTRL and trials_ds[t]['labels'][ix] != ctrl_class:\n",
    "            #print(ix, self.obs_labels[ix], '-> 3')\n",
    "            trials_ds[t]['labels'][ix] = ctrl_class\n",
    "            count_relabel += 1\n",
    "    print(t, \": Tot. samples relabeled (for every class):\", count_relabel)\n",
    "    pre_unique_lbls, pre_unique_cnts = np.unique(trials_ds[t]['labels'], return_counts=True)\n",
    "    print(t, \": # samps. per label (after relabeling)\\n\\t Labels:\", pre_unique_lbls[:4], \"Count:\", pre_unique_cnts[:4])\n",
    "\n",
    "\n",
    "print(\"Control classes removed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embb_mask = trials_ds['valid']['labels'] == 0\n",
    "mmtc_mask = trials_ds['valid']['labels'] == 1\n",
    "urllc_mask = trials_ds['valid']['labels'] == 2\n",
    "ctrl_mask = trials_ds['valid']['labels'] == 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducing dataset to 2 dimensions visualization using t-SNE method\n",
    "\n",
    "\n",
    "\n",
    "In many data analysis tasks, we deal with high-dimensional data. For example, each datapoint in our dataset is represented as a vector of size (4, 17), where 4 comes from the window size and 17 represents the number of feature columns. This means that each datapoint can be viewed as a point in a 4x17 dimensional space. But how can we interpret this data in a lower-dimensional space, such as 2D, for better understanding and visualization?\n",
    "\n",
    "Consider a simple analogy: imagine we have points in 3D space. We can project each point onto a 2D plane within that 3D space, and plot these projections on a 2D surface. This gives us a way to visualize and interpret our 3D data in a much simpler, 2D format.\n",
    "\n",
    "This idea of reducing high-dimensional data into a more interpretable, lower-dimensional representation is exactly what techniques like t-SNE aim to achieve. t-SNE models pairwise similarities between data points using probability distributions and minimizes the divergence between high-dimensional and low-dimensional representations. t-SNE captures complex non-linear relationships, making it effective for clustering and revealing hidden structures in data.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_embedded = TSNE(n_components=2, learning_rate='auto', init='random', perplexity=3).fit_transform(trials_ds['valid']['samples']['norm'].reshape((trials_ds['valid']['samples']['norm'].shape[0], -1)))\n",
    "\n",
    "plt.figure(figsize=(16, 12))\n",
    "plt.scatter(X_embedded[embb_mask, 0], X_embedded[embb_mask, 1], marker='o', c='red', label='embb')\n",
    "plt.scatter(X_embedded[mmtc_mask, 0], X_embedded[mmtc_mask, 1], marker='o', c='blue', label='mmtc')\n",
    "plt.scatter(X_embedded[urllc_mask, 0], X_embedded[urllc_mask, 1], marker='o', c='green', label='urllc')\n",
    "plt.scatter(X_embedded[ctrl_mask, 0], X_embedded[ctrl_mask, 1], marker='o', c='black', label='cntrl')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have processed our data; labeled it, cleaned it, removed unnessary columns and visualized it. The next step step is to define the maching learning model, train it with the training set and evluate it with the evaluation set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define some of the parameters of the model training process\n",
    "like the number of iterations we want to refine our ml model defined as `train_config['epochs']` \n",
    "or the `window_size` of our model or number of features of our model\n",
    "which is equal to number of columns of our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_config = {}\n",
    "train_config['Nclass'] = len(np.unique(trials_ds['train']['labels']))\n",
    "train_config['num_feats'] = trials_ds['train']['samples']['norm'].shape[2]\n",
    "train_config['logdir'] = \"./results\"\n",
    "train_config['window_size'] = window_size\n",
    "train_config['patience'] = 5                                # Num of epochs to wait before interrupting training with early stopping\n",
    "train_config['lr'] = 0.001\n",
    "train_config['lrmin'] = 0.00001\n",
    "train_config['lrpatience'] = 30\n",
    "train_config['batch_size'] = 512\n",
    "train_config['epochs'] = 20\n",
    "train_config['device'] = torch.device(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batching the data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class ORANTracesDataset(Dataset):\n",
    "    def __init__(self, obs_input, obs_labels):\n",
    "        self.obs_input, self.obs_labels = obs_input, obs_labels\n",
    "\n",
    "    def info(self):\n",
    "        unique_labels, unique_count = np.unique(np.array(self.obs_labels), return_counts=True)\n",
    "        ds_info = {\n",
    "            'numfeats': self.obs_input.shape[2],\n",
    "            'window_size': self.obs_input.shape[1],\n",
    "            'numsamps': self.obs_input.shape[0],\n",
    "            'nclasses': len(unique_labels),\n",
    "            'samples_per_class': unique_count\n",
    "        }\n",
    "        return ds_info\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.obs_input)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        obs = self.obs_input[idx, :, :]\n",
    "        label = self.obs_labels[idx]\n",
    "\n",
    "        try:\n",
    "            if self.transform:\n",
    "                obs = self.transform(obs)\n",
    "            if self.target_transform:\n",
    "                label = self.target_transform(label)\n",
    "        except:\n",
    "            pass\n",
    "        return obs, label\n",
    "\n",
    "ds_train = ORANTracesDataset(trials_ds['train']['samples']['norm'], trials_ds['train']['labels'])\n",
    "ds_test = ORANTracesDataset(trials_ds['valid']['samples']['norm'], trials_ds['valid']['labels'])\n",
    "train_dataloader = DataLoader(ds_train, batch_size=train_config['batch_size'], shuffle=True)\n",
    "test_dataloader = DataLoader(ds_test, batch_size=train_config['batch_size'], shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### **Overview**\n",
    "\n",
    "This machine learning model is a **function estimator** that takes a set of input data points, processes them through several layers of computation, and outputs a prediction. In simple terms, this model learns the relationship between the input data and a set of categories (classes). It does this by processing the input through a series of mathematical operations (layers) and then predicting the class probabilities.\n",
    "\n",
    "\n",
    "The architecure of the machine learning model is as follows. The model takes an input tensor of shape `(4, 17)`, for a `window_size=4` and the output is a set of 4 values representing the **probabilities** that the input data point belongs to one of the 4 possible classes.\n",
    "\n",
    "\n",
    "![Model Image](./images/model.jpg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ConvNN(nn.Module):\n",
    "    def __init__(self, numChannels=1, window_size=4, num_feats=17, classes=4):\n",
    "        super(ConvNN, self).__init__()\n",
    "        self.numChannels = numChannels\n",
    "\n",
    "        # initialize first set of CONV => RELU => POOL layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=numChannels, out_channels=20,\n",
    "                               kernel_size=(4, 1))\n",
    "        self.relu1 = nn.ReLU()\n",
    "\n",
    "        # pass a random input just to figure our the size of output layer\n",
    "        rand_x = torch.Tensor(np.random.random((1, 1, window_size, num_feats)))\n",
    "        output_size = torch.flatten(self.conv1(rand_x)).shape\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=output_size.numel(), out_features=512)\n",
    "\n",
    "        # self.fc1 = nn.Linear(in_features=20 * num_feats, out_features=512)\n",
    "\n",
    "        \n",
    "        self.relu3 = nn.ReLU()\n",
    "        # initialize our softmax classifier\n",
    "        self.fc2 = nn.Linear(in_features=512, out_features=classes)\n",
    "        self.logSoftmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.reshape \\\n",
    "            ((x.shape[0], self.numChannels, x.shape[1], x.shape[2]))   # CNN 2D expects a [N, Cin, H, W] size of data\n",
    "        # pass the input through our first set of CONV => RELU =>\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        ## flatten the output from the previous layer and pass it\n",
    "        ## through our only set of FC => RELU layers\n",
    "        x = torch.flatten(x, 1)       \n",
    "        x = self.fc1(x)\n",
    "        x = self.relu3(x)\n",
    "        # pass the output to our softmax classifier to get our output\n",
    "        # predictions\n",
    "        x = self.fc2(x)\n",
    "        output = self.logSoftmax(x)\n",
    "        # return the output predictions\n",
    "        return output\n",
    "\n",
    "model = ConvNN(classes=train_config['Nclass'], window_size=train_config['window_size'], \n",
    "                                     num_feats=train_config['num_feats']).to(train_config['device'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "\n",
    "data_point_dimensions = (trials_in.shape[1], trials_in.shape[2])\n",
    "print(\"Our one data point dimensions:\", data_point_dimensions)\n",
    "summary(model, input_size=(1, data_point_dimensions[0], data_point_dimensions[1]), verbose=0, depth=1)  # Adjust input_size based on your model input shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- #### Loss function\n",
    "\n",
    "Negative Log-Likelihood Loss is a loss function commonly used for classification tasks. It computes the loss by taking the negative log of the predicted probability assigned to the correct class. This loss is typically used after applying `torch.nn.LogSoftmax()`, which outputs log-probabilities.\n",
    "\n",
    "### Mathematical Definition:\n",
    "\n",
    "Given a batch of size $N$ with $C$ classes, let:\n",
    "\n",
    "- $x_i \\in \\mathbb{R}^C$ be the log-probabilities for sample $i$.\n",
    "- $y_i$ be the target class index for sample $i$ (where $y_i \\in \\{0, 1, \\dots, C-1\\}$).\n",
    "- $w_{y_i}$ be an optional weight for class $y_i$.\n",
    "\n",
    "The NLL loss is computed as:\n",
    "\n",
    "$$\n",
    "\\text{NLLLoss} = -\\frac{1}{N} \\sum_{i=1}^{N} w_{y_i} \\cdot x_{i, y_i}\n",
    "$$\n",
    "\n",
    "where $x_{i, y_i}$ is the log-probability of the correct class for sample $i$.\n",
    "\n",
    "### Key Points:\n",
    "- Requires input to be log-probabilities (not raw scores).\n",
    "- Often used with `LogSoftmax()`, as it outputs log-probabilities.\n",
    "- Can be weighted to address class imbalance.\n",
    "- Supports ignoring specific class indices during training.\n",
    "\n",
    "If no weights are provided, the loss simplifies to:\n",
    "\n",
    "$$\n",
    "\\text{NLLLoss} = -\\frac{1}{N} \\sum_{i=1}^{N} x_{i, y_i}\n",
    "$$\n",
    " -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function\n",
    "\n",
    "In machine learning, our model can be seen as a function estimator $ f $, which has parameters that we need to optimize. To do this, we define a **loss function** that quantifies how far off the model’s predictions are from the true values. For every input data point $ x $, the model outputs $ f(x) = [p_1, p_2, p_3, p_4] $, where each $ p_i $ represents the predicted probability for class $ i $. We then proceed to calculate a loss based on this output, then we apply **gradient descent** to adjust the model’s parameters and minimize the loss, ultimately improving the model’s ability to make accurate predictions. We use **Negative Log-Likelihood Loss (NLLLoss)** as the Loss function.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The **Negative Log-Likelihood Loss (NLLLoss)** is calculated as follows: Given the true label is $ y $ for the given input $ x $,  the NLLLoss is computed as:\n",
    "\n",
    "$$\n",
    "\\text{Loss} = -\\log(p_y)\n",
    "$$\n",
    "\n",
    "where $ p_y $ is the predicted probability corresponding to the true class $ y $. This loss is then averaged over all input data points in the batch, and the model is optimized to minimize this value, effectively increasing the predicted probability for the correct class during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy with the randomly initialized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have not trained the model so far. The model is only pre populated with initial values\n",
    "# Now we evaluate the untrained model on the validation dataset.\n",
    "\n",
    "\n",
    "model.eval()\n",
    "test_loss, correct = 0, 0\n",
    "test_dataset_length = len(test_dataloader.dataset)\n",
    "test_num_batches = len(test_dataloader)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X, y in test_dataloader:\n",
    "        X = X.to(train_config['device'])\n",
    "        y = y.to(train_config['device'])\n",
    "        pred = model(X)\n",
    "        test_loss += loss_fn(pred, y).item()\n",
    "        correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "test_loss /= test_num_batches\n",
    "correct /= test_dataset_length\n",
    "print(f\"Validation loss: {test_loss:>8f} , Validation accuracy: {(100 * correct):>0.1f}% \\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-family: Arial, sans-serif; color: #222; line-height: 1.6; max-width: 700px; margin: 20px auto; padding: 20px; border: 1px solid #444; border-radius: 8px; background-color: #eaeaea; box-shadow: 0px 4px 8px rgba(0, 0, 0, 0.15);\">\n",
    "  <h2 style=\"color: #ffffff; background-color: #333333; padding: 10px; border-radius: 4px; margin-bottom: 20px; text-align: center;\">Question: The model accuracy</h2>\n",
    "  \n",
    "  <p style=\"font-size: 1.1em; margin-top: 15px; margin-bottom: 20px;\">\n",
    "      Look at the output of the cell above. We are testing the testing the model on the validation set, without having trained the model. Validation accuracy, is the percentage of correct predictions on the validation dataset.      \n",
    "      As you can see, the validation accuracy is not a number near 0. But we haven't even trained the model. This means model parameters are just randomly selected and not tuned to correctly predict the class given the input. But why do we not get a numer close to zero?\n",
    "  </p>\n",
    "\n",
    "\n",
    "  <h3 style=\"color: #ff9800; margin-bottom: 15px;\">Hint</h3>\n",
    "  <p style=\"font-size: 1.05em; margin-bottom: 20px;\">\n",
    "      It is true that the model parameters are random. Hence, the output probabilites of the model are random and so the predicted class of the input is also a random number from 1 to 4. However, think of yourself writing a quiz of multi-choice questions with 4 possible answers for every question. How many correct answers would you get if you randomly choose the answers?\n",
    "  </p>\n",
    "  \n",
    "  <h4 style=\"color: #333333; margin-bottom: 10px; border-bottom: 1px solid #999; padding-bottom: 5px;\">Solution:</h4>\n",
    "  <p style=\"font-size: 1.05em; margin-bottom: 20px;\">\n",
    "      The answer is almost said in the hint section. You will get a fourth of the answers correctly!\n",
    "  </p>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training\n",
    "\n",
    "<!-- \n",
    "1. The Adam optimizer is used for model training.\n",
    "\n",
    "2. A learning rate scheduler (`ReduceLROnPlateau`) monitors the validation loss and reduces the learning rate when it stops improving. If the learning rate decreases, the model reverts to the last best state to prevent instability.\n",
    "\n",
    "\n",
    "3. In each training step: we compute model predictions, calculates the loss, perform backpropagation, and update the model parameters using the optimizer.\n",
    "\n",
    "4. In each validation step, we use `torch.no_grad()` to disable gradient computation (reducing memory usage and computation time), compute validation loss, accuracy, and update the confusion matrix.\n",
    "\n",
    "5. If the current model achieves a lower validation loss than best_loss, it is saved as the best model. The confusion matrix is also saved.\n",
    "\n",
    "6. If the model does not improve for `train_config['patience']` epochs, training is stopped early to prevent overfitting.\n",
    "\n",
    " -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=train_config['lr'])\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=train_config['lrpatience'], min_lr=train_config['lrmin'], \n",
    "                              verbose=True)\n",
    "\n",
    "loss_results = []\n",
    "best_loss = np.inf\n",
    "epochs_wo_improvement = 0\n",
    "times = []\n",
    "last_lr = np.inf\n",
    "model_name = f\"model.cnn.{train_config['window_size']}.pt\"\n",
    "train_dataset_length = len(train_dataloader.dataset)\n",
    "train_num_batches = len(train_dataloader)\n",
    "\n",
    "# Training Epochs\n",
    "for e in range(train_config['epochs']):\n",
    "    # Train Step\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    loss_train, correct_train = 0, 0\n",
    "    for batch, (X, y) in enumerate(train_dataloader):\n",
    "        # Compute prediction error\n",
    "        X = X.to(train_config['device'])\n",
    "        y = y.to(train_config['device'])\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_train += loss.item()\n",
    "        correct_train += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    loss_train /= train_num_batches\n",
    "    correct_train /= train_dataset_length\n",
    "    print(f\"\\nTrainig loss: {loss_train:>7f}, Trainig accuracy: {(100 * correct_train):>0.1f}% \\n\")\n",
    "\n",
    "    times.append(time.time() - start_time)\n",
    "\n",
    "    # Test step\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    conf_matrix = np.zeros((train_config['Nclass'], train_config['Nclass']))\n",
    "    with torch.no_grad():\n",
    "        for X, y in test_dataloader:\n",
    "            X = X.to(train_config['device'])\n",
    "            y = y.to(train_config['device'])\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "            conf_matrix += conf_mat(y.cpu(), pred.argmax(1).cpu(), labels=list(range(train_config['Nclass'])))\n",
    "    test_loss /= test_num_batches\n",
    "    correct /= test_dataset_length\n",
    "    loss, cm = test_loss, conf_matrix\n",
    "    print(f\"Validation loss: {test_loss:>8f} , Validation accuracy: {(100 * correct):>0.1f}% \\n\")\n",
    "\n",
    "    # Learning rate adaption\n",
    "    scheduler.step(loss)\n",
    "    if scheduler._last_lr[0] < last_lr:    # detect change in lr by scheduler\n",
    "        if last_lr == np.inf:\n",
    "            last_lr = scheduler._last_lr[0]\n",
    "        else:\n",
    "            print(\"[lr change detect] -> Reloading from best state, skipping epoch\")\n",
    "            last_lr = scheduler._last_lr[0]\n",
    "            # Load the last best state for the model if the scheduler has changed.\n",
    "            # This is to address large instability while training with higher learning rates and avoid\n",
    "            # continuing from a worse state that might lead to a lower local optima\n",
    "            model.load_state_dict(torch.load(os.path.join(train_config['logdir'], model_name), map_location=train_config['device'])['model_state_dict'])\n",
    "            # skip checks and go to next epoch\n",
    "            continue\n",
    "\n",
    "    loss_results.append(loss)\n",
    "    epochs_wo_improvement += 1\n",
    "    if best_loss > loss:\n",
    "        pickle.dump(cm, open(os.path.join(train_config['logdir'], f\"conf_matrix.cnn.{train_config['window_size']}.last.pkl\"), 'wb'))\n",
    "        epochs_wo_improvement = 0\n",
    "        best_loss = loss\n",
    "        torch.save({\n",
    "             'model_state_dict': model.state_dict(),\n",
    "             'optimizer_state_dict': optimizer.state_dict(),\n",
    "             'loss': loss,\n",
    "        }, os.path.join(train_config['logdir'], model_name))\n",
    "\n",
    "    if epochs_wo_improvement > train_config['patience']: # early stopping\n",
    "        print('------------------------------------')\n",
    "        print('Early termination implemented at epoch:', e+1)\n",
    "        print('------------------------------------')\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Window Size Impact\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### A. Fill the table below by training different models with different window sizes. \n",
    "\n",
    "change the window_size in the beginning cells of this notebook. Then run the all cells again from top to bottom. To run all cells, Look at the top bar and select Run->Run All cells. \n",
    "\n",
    "\n",
    "|  | window size = 4| window size = 16| window size = 32 |\n",
    "|----------|----------|----------|----------|\n",
    "| CNN Training Accuracy| result 1 | result 3 | result 5 |\n",
    "| CNN Validation Accuracy| result 2 | result 4 | result 6 |\n",
    "\n",
    "\n",
    "#### B. What is the impact of window size on the performance?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
